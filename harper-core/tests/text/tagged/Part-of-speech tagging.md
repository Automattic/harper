> <!--
# Unlintable
>            source: https://en.wikipedia.org/w/index.php?title=Part-of-speech_tagging&oldid=1275774341
# Unlintable Unlintable
>            license: CC BY-SA 4.0
# Unlintable Unlintable
>            -->
# Unlintable Unlintable
>            Part  - of - speech tagging
# Unlintable N/V/J . P  . N/V+   N/V
>
#
> In      corpus linguistics , part  - of - speech tagging ( POS tagging or    PoS tagging or
# NPr/J/P N+     N           . N/V/J . P  . N/V    N/V     . N+  N/V     NPr/C N+  N/V     NPr/C
> POST     ) , also called grammatical tagging is the process of marking up      a   word in      a
# NPr/V/P+ . . W?   V/J    J           N/V     VL D   N/V     P  N/V     N/V/J/P D/P N/V  NPr/J/P D/P
> text ( corpus ) as  corresponding to a   particular part  of speech , based on  both its
# N/V  . N+     . N/R N/V/J         P  D/P N/J        N/V/J P  N/V+   . V/J   J/P I/C  I/D$+
> definition and its   context . A   simplified form of this    is commonly taught to
# N          V/C I/D$+ N/V+    . D/P V/J        N/V  P  I/Ddem+ VL R        V      P
> school - age children , in      the identification of words  as  nouns , verbs  , adjectives ,
# N/V    . N/V NPl      . NPr/J/P D   N              P  NPl/V+ N/R NPl/V . NPl/V+ . NPl/V      .
> adverbs , etc.
# NPl/V   . W?
>
#
> Once performed by    hand , POS tagging is now       done  in      the context of computational
# N/C  V/J       N/J/P N/V+ . N+  N/V     VL NPr/V/J/C N/V/J NPr/J/P D   N/V     P  J+
> linguistics , using algorithms which associate discrete terms  , as  well  as  hidden
# N+          . V     NPl+       I/C+  N/V/J+    J        NPl/V+ . N/R N/V/J N/R V/J
> parts of speech , by    a   set     of descriptive tags   . POS - tagging algorithms fall into
# NPl/V P  N/V+   . N/J/P D/P NPr/V/J P  N/J+        NPl/V+ . N+  . N/V     NPl        N/V  P
> two distinctive groups : rule - based and  stochastic . E. Brill's tagger , one     of the
# N   N/J         NPl/V+ . N/V+ . V/J+  V/C+ J+         . ?  ?       N      . N/I/V/J P  D
> first and most  widely used English  POS - taggers , employs rule - based algorithms .
# N/V/J V/C N/I/J R      V/J  NPr/V/J+ N+  . NPl     . NPl/V   N/V+ . V/J   NPl+       .
>
#
> Principle
# N/V
>
#
> Part  - of - speech tagging is harder than just having a   list of words and their
# N/V/J . P  . N/V    N/V     VL J      C/P  V/J  V      D/P N/V  P  NPl/V V/C D$+
> parts of speech , because some   words  can    represent more      than one     part  of speech
# NPl/V P  N/V+   . C/P     I/J/R+ NPl/V+ NPr/VX V         NPr/I/V/J C/P  N/I/V/J N/V/J P  N/V+
> at  different times  , and because some  parts of speech are complex . This    is not
# N/P N/J+      NPl/V+ . V/C C/P     I/J/R NPl/V P  N/V+   V+  N/V/J+  . I/Ddem+ VL N/C
> rare  — in      natural languages ( as  opposed to many    artificial languages ) , a   large
# N/V/J . NPr/J/P N/J     NPl/V+    . N/R V/J     P  N/I/J/D J          NPl/V+    . . D/P N/J
> percentage of word - forms  are ambiguous . For example , even  " dogs   " , which is
# N          P  N/V+ . NPl/V+ V+  J+        . C/P N/V+    . N/V/J . NPl/V+ . . I/C+  VL
> usually thought of as  just a    plural noun , can    also be   a   verb :
# R       N/V     P  N/R V/J  D/P+ N/J+   N/V+ . NPr/VX W?   N/VX D/P N/V+ .
>
#
> The sailor dogs  the hatch .
# D+  N      NPl/V D   N/V+  .
>
#
> Correct grammatical tagging will   reflect that        " dogs   " is here  used as  a    verb , not
# N/V/J+  J           N/V     NPr/VX V       N/I/C/Ddem+ . NPl/V+ . VL N/J/R V/J  N/R D/P+ N/V+ . N/C
> as  the more      common plural noun . Grammatical context is one     way  to determine
# N/R D   NPr/I/V/J N/V/J  N/J    N/V+ . J           N/V+    VL N/I/V/J N/J+ P  V
> this    ; semantic analysis can    also be   used to infer that        " sailor " and " hatch "
# I/Ddem+ . N/J      N+       NPr/VX W?   N/VX V/J  P  J     N/I/C/Ddem+ . N+     . V/C . N/V   .
> implicate " dogs  " as  1 ) in      the nautical context and 2 ) an  action applied to the
# N/V       . NPl/V . N/R # . NPr/J/P D+  J+       N/V+    V/C # . D/P N/V/J+ V/J     P  D
> object " hatch " ( in      this    context , " dogs   " is a   nautical term   meaning " fastens ( a
# N/V+   . N/V   . . NPr/J/P I/Ddem+ N/V+    . . NPl/V+ . VL D/P J        N/V/J+ N/V/J+  . V       . D/P
> watertight door ) securely " ) .
# J          N/V+ . R        . . .
>
#
> Tag  sets
# N/V+ NPl/V
>
#
> Schools commonly teach that       there are 9 parts of speech in      English : noun , verb ,
# NPl/V+  R        N/V   N/I/C/Ddem +     V   # NPl/V P  N/V+   NPr/J/P NPr/V/J . N/V+ . N/V+ .
> article , adjective , preposition , pronoun , adverb , conjunction , and interjection .
# N/V+    . N/V/J+    . N/V         . N/V+    . N/V+   . N/V+        . V/C N+           .
> However , there are clearly many    more      categories and sub   - categories . For nouns ,
# C       . +     V   R       N/I/J/D NPr/I/V/J NPl+       V/C N/V/P . NPl        . C/P NPl/V .
> the plural , possessive , and singular forms  can     be    distinguished . In      many
# D   N/J    . N/J        . V/C N/J      NPl/V+ NPr/VX+ N/VX+ V/J+          . NPr/J/P N/I/J/D+
> languages words  are also marked for their " case   " ( role as  subject , object ,
# NPl/V+    NPl/V+ V   W?   V/J    C/P D$+   . NPr/V+ . . N    N/R N/V/J   . N/V+   .
> etc. ) , grammatical gender , and so      on  ; while   verbs  are marked for tense , aspect ,
# +    . . J+          N/V/J+ . V/C N/I/J/C J/P . N/V/C/P NPl/V+ V   V/J    C/P N/V/J . N/V+   .
> and other  things . In      some   tagging systems , different inflections of the same
# V/C N/V/J+ NPl/V+ . NPr/J/P I/J/R+ N/V     NPl+    . N/J       NPl         P  D+  I/J+
> root   word will   get different parts of speech , resulting in      a   large number of
# NPr/V+ N/V+ NPr/VX N/V N/J       NPl/V P  N/V+   . V         NPr/J/P D/P N/J   N/V/J  P+
> tags   . For example , NN for singular common nouns , NNS for plural common nouns , NP
# NPl/V+ . C/P N/V+    . ?  C/P N/J      N/V/J+ NPl/V . ?   C/P N/J    N/V/J+ NPl/V . NPr
> for singular proper nouns ( see the POS tags   used in      the Brown    Corpus ) . Other
# C/P N/J      N/J    NPl/V . N/V D+  N+  NPl/V+ V/J  NPr/J/P D+  NPr/V/J+ N+     . . N/V/J
> tagging systems use a   smaller number of tags   and ignore fine  differences or
# N/V     NPl+    N/V D/P J       N/V/J  P  NPl/V+ V/C V      N/V/J N/V         NPr/C
> model  them   as   features somewhat independent from part  - of - speech .
# N/V/J+ N/IPl+ N/R+ NPl/V+   N/I      N/J         P    N/V/J . P  . N/V+   .
>
#
> In      part  - of - speech tagging by    computer , it     is typical to distinguish from 50 to
# NPr/J/P N/V/J . P  . N/V    N/V     N/J/P N/V+     . NPr/I+ VL N/J     P  V           P    #  P
> 150 separate parts of speech for English  . Work on  stochastic methods for tagging
# #   N/V/J    NPl/V P  N/V    C/P NPr/V/J+ . N/V  J/P J          NPl/V   C/P N/V
> Koine Greek   ( DeRose 1990 ) has used over    1 , 000 parts of speech and found that
# ?     NPr/V/J . ?      #    . V   V/J  N/V/J/P # . #   NPl/V P  N/V+   V/C N/V   N/I/C/Ddem
> about as  many     words  were ambiguous in      that        language as  in      English  . A
# J/P   N/R N/I/J/D+ NPl/V+ N/V  J         NPr/J/P N/I/C/Ddem+ N/V+     N/R NPr/J/P NPr/V/J+ . D/P
> morphosyntactic descriptor in      the case  of morphologically rich    languages is
# ?               N          NPr/J/P D   NPr/V P  ?               NPr/V/J NPl/V+    VL
> commonly expressed using very short      mnemonics , such as  Ncmsan for Category = Noun ,
# R        V/J       V     J/R  NPr/V/J/P+ NPl       . N/I  N/R ?      C/P N        . N/V+ .
> Type = common , Gender = masculine , Number = singular , Case  = accusative , Animate
# N/V  . N/V/J  . N/V/J  . N/J       . N/V/J  . N/J      . NPr/V . N/J        . V/J
> = no    .
# . NPr/P .
>
#
> The most  popular " tag  set     " for POS tagging for American English  is probably the
# D   N/I/J N/J     . N/V+ NPr/V/J . C/P N+  N/V     C/P NPr/J    NPr/V/J+ VL R        D+
> Penn tag  set     , developed in      the Penn Treebank project . It     is largely similar to
# NPr+ N/V+ NPr/V/J . V/J       NPr/J/P D+  NPr+ ?        N/V+    . NPr/I+ VL R       N/J     P
> the earlier Brown   Corpus and LOB Corpus tag  sets  , though much   smaller . In
# D   J       NPr/V/J N      V/C N/V N+     N/V+ NPl/V . V/C    N/I/J+ J+      . NPr/J/P
> Europe , tag  sets  from the Eagles Guidelines see wide use  and include versions
# NPr+   . N/V+ NPl/V P    D+  NPl/V+ NPl+       N/V N/J  N/V+ V/C N/V     NPl/V
> for multiple languages .
# C/P N/J+     NPl/V+    .
>
#
> POS tagging work has been done  in      a   variety of languages , and the set     of POS
# N+  N/V     N/V+ V   N/V  N/V/J NPr/J/P D/P N       P  NPl/V+    . V/C D   NPr/V/J P  N+
> tags   used varies greatly with language . Tags   usually are designed to include
# NPl/V+ V/J  NPl/V  R       P    N/V+     . NPl/V+ R       V   V/J      P  N/V
> overt morphological distinctions , although this    leads to inconsistencies such as
# N/J   J+            NPl+         . C        I/Ddem+ NPl/V P  NPl             N/I  N/R
> case   - marking for pronouns but   not nouns in      English  , and much  larger
# NPr/V+ . N/V     C/P NPl/V    N/C/P N/C NPl/V NPr/J/P NPr/V/J+ . V/C N/I/J J
> cross      - language differences . The tag  sets  for heavily inflected languages such as
# NPr/V/J/P+ . N/V+     N/V         . D+  N/V+ NPl/V C/P R       V/J       NPl/V+    N/I  N/R
> Greek   and Latin can    be   very large ; tagging words  in      agglutinative languages such
# NPr/V/J V/C NPr/J NPr/VX N/VX J/R  N/J   . N/V     NPl/V+ NPr/J/P ?             NPl/V+    N/I
> as  Inuit languages may    be   virtually impossible . At  the other  extreme , Petrov et
# N/R NPr/J NPl/V+    NPr/VX N/VX R+        N/J+       . N/P D+  N/V/J+ N/J     . ?      ?
> al. have proposed a   " universal " tag  set     , with 12 categories ( for example , no
# ?   N/VX V/J      D/P . N/J       . N/V+ NPr/V/J . P    #  NPl        . C/P N/V+    . NPr/P
> subtypes of nouns , verbs  , punctuation , and so      on   ) . Whether a   very small   set     of
# NPl      P  NPl/V . NPl/V+ . N+          . V/C N/I/J/C J/P+ . . I/C     D/P J/R  NPr/V/J NPr/V/J P
> very broad tags  or    a   much  larger set     of more      precise ones   is preferable , depends
# J/R  N/J   NPl/V NPr/C D/P N/I/J J      NPr/V/J P  NPr/I/V/J V/J     NPl/V+ VL W?         . NPl/V
> on  the purpose at  hand . Automatic tagging is easier on  smaller tag  - sets   .
# J/P D+  N/V     N/P N/V+ . N/J       N/V     VL J      J/P J       N/V+ . NPl/V+ .
>
#
> History
# N
>
#
> The Brown    Corpus
# D   NPr/V/J+ N
>
#
> Research on  part  - of - speech tagging has been closely tied to corpus linguistics .
# N/V      J/P N/V/J . P  . N/V    N/V     V   N/V  R       V/J  P  N      N+          .
> The first major   corpus of English  for computer analysis was the Brown   Corpus
# D   N/V/J NPr/V/J N      P  NPr/V/J+ C/P N/V+     N+       V   D   NPr/V/J N
> developed at  Brown   University by    Henry Kučera and W. Nelson Francis , in      the
# V/J       N/P NPr/V/J N          N/J/P NPr+  ?      V/C ?  NPr+   NPr+    . NPr/J/P D
> mid    - 1960s . It     consists of about 1 , 000 , 000 words of running English  prose text ,
# N/J/P+ . #d    . NPr/I+ NPl/V    P  J/P   # . #   . #   NPl/V P  N/V/J/P NPr/V/J+ N/V   N/V+ .
> made up      of 500 samples from randomly chosen publications . Each sample is 2 , 000
# N/V  N/V/J/P P  #   NPl/V+  P    R+       V/J    NPl+         . D+   N/V+   VL # . #
> or    more      words  ( ending at  the first  sentence - end after 2 , 000 words  , so      that       the
# NPr/C NPr/I/V/J NPl/V+ . N/V    N/P D   N/V/J+ N/V+     . N/V J/P   # . #   NPl/V+ . N/I/J/C N/I/C/Ddem D+
> corpus contains only  complete sentences ) .
# N+     V        J/R/C N/V/J+   NPl/V+    . .
>
#
> The Brown   Corpus was painstakingly " tagged " with part  - of - speech markers over
# D+  NPr/V/J N      V   R             . V/J    . P    N/V/J . P  . N/V    NPl/V   N/V/J/P
> many     years . A    first  approximation was done  with a   program by    Greene and Rubin ,
# N/I/J/D+ NPl+  . D/P+ N/V/J+ N+            V   N/V/J P    D/P NPr/V   N/J/P NPr    V/C NPr   .
> which consisted of a   huge handmade list of what categories could co       - occur at
# I/C+  V/J       P  D/P J    N/J      N/V  P  N/I+ NPl+       N/VX  NPr/I/V+ . V     N/P+
> all     . For example , article then  noun can    occur , but   article then  verb ( arguably )
# N/I/J/C . C/P N/V+    . N/V+    N/J/C N/V+ NPr/VX V     . N/C/P N/V+    N/J/C N/V+ . R        .
> cannot . The program got about 70 % correct . Its   results were repeatedly reviewed
# N/V    . D+  NPr/V+  V   J/P   #  . N/V/J+  . I/D$+ NPl/V+  N/V  R          V/J
> and corrected by    hand , and later users sent in      errata so      that        by    the late 70 s
# V/C V/J       N/J/P N/V+ . V/C J     NPl+  N/V  NPr/J/P N      N/I/J/C N/I/C/Ddem+ N/J/P D   N/J  #  ?
> the tagging was nearly perfect ( allowing for some  cases  on  which even  human
# D   N/V     V   R      N/V/J   . V        C/P I/J/R NPl/V+ J/P I/C+  N/V/J N/V/J
> speakers might  not agree ) .
# +        N/VX/J N/C V     . .
>
#
> This    corpus has been used for innumerable studies of word - frequency and of
# I/Ddem+ N      V   N/V  V/J  C/P J           NPl/V   P  N/V+ . N         V/C P
> part  - of - speech and inspired the development of similar " tagged " corpora in      many
# N/V/J . P  . N/V    V/C V/J      D   N           P  N/J     . V/J    . NPl     NPr/J/P N/I/J/D+
> other  languages . Statistics derived by    analyzing it     formed the basis for most
# N/V/J+ NPl/V+    . NPl/V+     V/J     N/J/P V         NPr/I+ V/J    D   N     C/P N/I/J
> later part  - of - speech tagging systems , such as  CLAWS  and VOLSUNGA . However , by
# J     N/V/J . P  . N/V    N/V     NPl     . N/I  N/R NPl/V+ V/C ?        . C       . N/J/P
> this    time   ( 2005 ) it     has been superseded by    larger corpora such as  the 100
# I/Ddem+ N/V/J+ . #    . NPr/I+ V   N/V  V/J        N/J/P J      NPl+    N/I  N/R D   #
> million word British National Corpus , even  though larger corpora are rarely so
# N       N/V+ NPr/J   N/J+     N+     . N/V/J V/C    J+     NPl+    V   R      N/I/J/C
> thoroughly curated .
# R+         V/J+    .
>
#
> For some  time  , part  - of - speech tagging was considered an  inseparable part  of
# C/P I/J/R N/V/J . N/V/J . P  . N/V    N/V     V   V/J        D/P N/J         N/V/J P
> natural language processing , because there are certain cases  where the correct
# N/J+    N/V+     V+         . C/P     +     V   I/J     NPl/V+ N/C   D   N/V/J
> part  of speech cannot be   decided without understanding the semantics or    even  the
# N/V/J P  N/V+   N/V    N/VX N/V/J   C/P     N/V/J+        D+  NPl       NPr/C N/V/J D
> pragmatics of the context . This    is extremely expensive , especially because
# NPl        P  D+  N/V+    . I/Ddem+ VL R         J         . R          C/P
> analyzing the higher levels is much  harder when  multiple part  - of - speech
# V         D+  J+     NPl/V+ VL N/I/J J      N/I/C N/J      N/V/J . P  . N/V
> possibilities must be   considered for each word .
# NPl           N/V  N/VX V/J        C/P D+   N/V+ .
>
#
> Use of hidden Markov models
# N/V P  V/J    NPr+   NPl/V
>
#
> In      the mid   - 1980s , researchers in      Europe began to use hidden Markov models ( HMMs )
# NPr/J/P D   N/J/P . #d    . W?          NPr/J/P NPr+   V     P  N/V V/J    NPr    NPl/V+ . ?    .
> to disambiguate parts of speech , when  working to tag the Lancaster - Oslo - Bergen
# P  V            NPl/V P  N/V+   . N/I/C V       P  N/V D   NPr       . NPr+ . NPr
> Corpus of British English  . HMMs involve counting cases ( such as  from the Brown
# N      P  NPr/J+  NPr/V/J+ . ?    V       V        NPl/V . N/I  N/R P    D+  NPr/V/J+
> Corpus ) and making a   table of the probabilities of certain sequences . For
# N+     . V/C N/V    D/P N/V   P  D   NPl           P  I/J+    NPl/V+    . C/P
> example , once you've seen an  article such as  ' the ' , perhaps the next   word is a
# N/V+    . N/C  W?     N/V  D/P N/V+    N/I  N/R . D   . . N       D+  N/J/P+ N/V+ VL D/P
> noun 40 % of the time   , an   adjective 40 % , and a    number 20 % . Knowing this    , a
# N/V  #  . P  D+  N/V/J+ . D/P+ N/V/J+    #  . . V/C D/P+ N/V/J+ #  . . N/V/J/P I/Ddem+ . D/P+
> program can    decide that        " can    " in      " the can    " is far   more      likely to be   a   noun than
# NPr/V+  NPr/VX V      N/I/C/Ddem+ . NPr/VX . NPr/J/P . D+  NPr/VX . VL N/V/J NPr/I/V/J N/J    P  N/VX D/P N/V  C/P
> a   verb or    a    modal . The same method can    , of course , be   used to benefit from
# D/P N/V  NPr/C D/P+ N/J+  . D+  I/J+ N/V+   NPr/VX . P  N/V+   . N/VX V/J  P  N/V     P
> knowledge about the following words .
# N/V+      J/P   D+  N/V/J/P+  NPl/V .
>
#
> More      advanced ( " higher - order " ) HMMs learn the probabilities not only  of pairs
# NPr/I/V/J V/J      . . J      . N/V   . . ?    N/V   D+  NPl+          N/C J/R/C P  NPl/V+
> but   triples or    even  larger sequences . So      , for example , if  you've just seen a
# N/C/P NPl/V   NPr/C N/V/J J      NPl/V+    . N/I/J/C . C/P N/V+    . N/C W?     V/J  N/V  D/P
> noun followed by    a    verb , the next   item may    be   very likely a   preposition ,
# N/V  V/J      N/J/P D/P+ N/V+ . D+  N/J/P+ N/V+ NPr/VX N/VX J/R  N/J    D/P N/V         .
> article , or    noun , but   much  less    likely another verb .
# N/V+    . NPr/C N/V+ . N/C/P N/I/J V/J/C/P N/J+   I/D     N/V  .
>
#
> When  several ambiguous words  occur together , the possibilities multiply .
# N/I/C J/D     J         NPl/V+ V     J        . D+  NPl           N/V+     .
> However , it     is easy  to enumerate every combination and to assign a   relative
# C       . NPr/I+ VL N/V/J P  V         D+    N+          V/C P  N/V    D/P N/J
> probability to each one      , by    multiplying together the probabilities of each
# N           P  D+   N/I/V/J+ . N/J/P V           J        D   NPl           P  D+
> choice in      turn . The combination with the highest probability is then   chosen . The
# N/J+   NPr/J/P N/V  . D   N           P    D+  +       N+          VL N/J/C+ V/J    . D+
> European group developed CLAWS  , a   tagging program that        did exactly this    and
# N/J+     N/V+  V/J       NPl/V+ . D/P N/V+    NPr/V+  N/I/C/Ddem+ V   R       I/Ddem+ V/C
> achieved accuracy in      the 93 – 95 % range .
# V/J      N+       NPr/J/P D   #  . #  . N/V+  .
>
#
> Eugene Charniak points out       in      Statistical techniques for natural language
# NPr+   ?        NPl/V+ N/V/J/R/P NPr/J/P J           NPl        C/P N/J     N/V+
> parsing ( 1997 ) that        merely assigning the most  common tag to each known word and
# V       . #    . N/I/C/Ddem+ R      V         D   N/I/J N/V/J  N/V P  D+   N/V/J N/V  V/C
> the tag  " proper noun " to all      unknowns will   approach 90 % accuracy because many
# D   N/V+ . N/J    N/V  . P  N/I/J/C+ NPl/V+   NPr/VX N/V      #  . N+       C/P     N/I/J/D+
> words  are unambiguous , and many     others only  rarely represent their less    - common
# NPl/V+ V   J           . V/C N/I/J/D+ NPl/V+ J/R/C R      V         D$+   V/J/C/P . N/V/J
> parts of speech .
# NPl/V P  N/V+   .
>
#
> CLAWS  pioneered the field of HMM - based part  of speech tagging but   was quite
# NPl/V+ V/J       D   N/V   P  V   . V/J   N/V/J P  N/V+   N/V     N/C/P V   N
> expensive since it     enumerated all      possibilities . It     sometimes had to resort to
# J         C/P   NPr/I+ V/J        N/I/J/C+ NPl+          . NPr/I+ R         V   P  N/V    P
> backup methods when  there were simply too many     options ( the Brown    Corpus
# N/J    NPl/V+  N/I/C +     N/V  R      W?  N/I/J/D+ NPl/V   . D+  NPr/V/J+ N+
> contains a   case  with 17 ambiguous words in      a    row  , and there are words  such as
# V        D/P NPr/V P    #  J         NPl/V NPr/J/P D/P+ N/V+ . V/C +     V   NPl/V+ N/I  N/R
> " still " that        can    represent as  many    as  7 distinct parts of speech .
# . N/V/J . N/I/C/Ddem+ NPr/VX V         N/R N/I/J/D N/R # V/J      NPl/V P  N/V+   .
>
#
> HMMs underlie the functioning of stochastic taggers and are used in      various
# ?    V        D   V           P  J          NPl     V/C V   V/J  NPr/J/P J
> algorithms one     of the most  widely used being the bi  - directional inference
# NPl+       N/I/V/J P  D   N/I/J R      V/J  N/V/C D   N/J . N/J         N+
> algorithm .
# N+        .
>
#
> Dynamic programming methods
# N/J+    N/V+        NPl/V
>
#
> In      1987 , Steven DeRose and Kenneth W. Church independently developed dynamic
# NPr/J/P #    . NPr+   ?      V/C NPr+    ?  NPr/V+ R             V/J       N/J
> programming algorithms to solve the same problem in      vastly less    time   . Their
# N/V+        NPl+       P  N/V   D   I/J  N/J     NPr/J/P R      V/J/C/P N/V/J+ . D$+
> methods were similar to the Viterbi algorithm known for some  time   in      other
# NPl/V+  N/V  N/J     P  D   ?       N         N/V/J C/P I/J/R N/V/J+ NPr/J/P N/V/J+
> fields   . DeRose used a   table of pairs  , while   Church used a   table of triples and a
# NPrPl/V+ . ?      V/J  D/P N/V   P  NPl/V+ . N/V/C/P NPr/V+ V/J  D/P N/V   P  NPl/V   V/C D/P
> method of estimating the values for triples that        were rare  or    nonexistent in      the
# N/V    P  V          D   NPl/V  C/P NPl/V   N/I/C/Ddem+ N/V  N/V/J NPr/C N/J         NPr/J/P D+
> Brown    Corpus ( an  actual measurement of triple probabilities would require a   much
# NPr/V/J+ N      . D/P N/J    N           P  N/V/J  NPl+          N/VX  N/V     D/P N/I/J
> larger corpus ) . Both methods achieved an  accuracy of over    95 % . DeRose's 1990
# J      N+     . . I/C  NPl/V+  V/J      D/P N        P  N/V/J/P #  . . ?        #
> dissertation at  Brown   University included analyses  of the specific error types  ,
# N+           N/P NPr/V/J N+         V/J      N/V/Au/Br P  D+  N/J+     N/V+  NPl/V+ .
> probabilities , and other  related data , and replicated his   work for Greek   , where
# NPl+          . V/C N/V/J+ J+      N+   . V/C V/J        I/D$+ N/V  C/P NPr/V/J . N/C
> it     proved similarly effective .
# NPr/I+ V/J    R+        N/J       .
>
#
> These   findings were surprisingly disruptive to the field of natural language
# I/Ddem+ N        N/V  R            J          P  D   N/V   P  N/J+    N/V+
> processing . The accuracy reported was higher than the typical accuracy of very
# V+         . D+  N+       V/J      V   J      C/P  D   N/J     N        P  J/R
> sophisticated algorithms that        integrated part  of speech choice with many    higher
# V/J           NPl+       N/I/C/Ddem+ V/J        N/V/J P  N/V+   N/J    P    N/I/J/D J
> levels of linguistic analysis : syntax , morphology , semantics , and so       on  . CLAWS ,
# NPl/V  P  J          N+       . N+     . N+         . NPl+      . V/C N/I/J/C+ J/P . NPl/V .
> DeRose's and Church's methods did fail  for some  of the known  cases  where
# ?        V/C N$       NPl/V+  V   N/V/J C/P I/J/R P  D+  N/V/J+ NPl/V+ N/C
> semantics is required , but   those   proved negligibly rare   . This    convinced many    in
# NPl+      VL V/J      . N/C/P I/Ddem+ V/J    R+         N/V/J+ . I/Ddem+ V/J       N/I/J/D NPr/J/P
> the field that        part  - of - speech tagging could usefully be   separated from the other
# D+  N/V+  N/I/C/Ddem+ N/V/J . P  . N/V    N/V     N/VX  R        N/VX V/J       P    D   N/V/J
> levels of processing ; this    , in      turn , simplified the theory and practice of
# NPl/V  P  V          . I/Ddem+ . NPr/J/P N/V  . V/J        D+  N      V/C N/V      P
> computerized language analysis and encouraged researchers to find ways to
# V/J          N/V+     N+       V/C V/J        +           P  N/V  NPl+ P
> separate other  pieces as   well  . Markov Models became the standard method for the
# N/V/J    N/V/J+ NPl/V+ N/R+ N/V/J . NPr    NPl/V+ V      D   N/J      N/V    C/P D
> part  - of - speech assignment .
# N/V/J . P  . N/V+   N+         .
>
#
> Unsupervised taggers
# V/J+         NPl
>
#
> The methods already discussed involve working from a   pre    - existing corpus to
# D+  NPl/V   W?      V/J       V       V       P    D/P N/V/P+ . V        N      P
> learn tag  probabilities . It     is , however , also possible to bootstrap using
# N/V   N/V+ NPl+          . NPr/I+ VL . C       . W?   N/J      P  N/V       V
> " unsupervised " tagging . Unsupervised tagging techniques use an  untagged corpus
# . V/J          . N/V     . V/J          N/V     NPl+       N/V D/P ?        N
> for their training data and produce the tagset by     induction . That        is , they
# C/P D$+   N/V+     N+   V/C N/V     D   N      N/J/P+ N         . N/I/C/Ddem+ VL . IPl+
> observe patterns in      word use , and derive part  - of - speech categories themselves .
# N/V     NPl/V+   NPr/J/P N/V+ N/V . V/C N/V    N/V/J . P  . N/V    NPl+       IPl+       .
> For example , statistics readily reveal that        " the " , " a   " , and " an  " occur in
# C/P N/V+    . NPl/V+     R       N/V    N/I/C/Ddem+ . D   . . . D/P . . V/C . D/P . V     NPr/J/P
> similar contexts , while   " eat " occurs in      very different ones   . With sufficient
# N/J+    NPl/V+   . N/V/C/P . N/V . V      NPr/J/P J/R  N/J+      NPl/V+ . P    J+
> iteration , similarity classes of words  emerge that        are remarkably similar to
# N         . N          NPl/V   P  NPl/V+ N/V    N/I/C/Ddem+ V   R          N/J     P
> those   human linguists would expect ; and the differences themselves sometimes
# I/Ddem+ N/V/J NPl+      N/VX  V      . V/C D+  N/V+        IPl+       R
> suggest valuable new    insights .
# V       N/J+     N/V/J+ NPl+     .
>
#
> These  two categories can    be   further subdivided into rule - based , stochastic , and
# I/Ddem N+  NPl        NPr/VX N/VX V/J     V/J        P    N/V  . V/J   . J          . V/C
> neural approaches .
# J+     NPl/V+     .
>
#
> Other  taggers and methods
# N/V/J+ NPl     V/C NPl/V
>
#
> Some   current major   algorithms for part  - of - speech tagging include the Viterbi
# I/J/R+ N/J     NPr/V/J NPl        C/P N/V/J . P  . N/V    N/V     N/V     D   ?
> algorithm , Brill tagger , Constraint Grammar , and the Baum - Welch algorithm ( also
# N         . N/J   N      . N+         N/V+    . V/C D   NPr  . ?     N         . W?
> known as  the forward - backward algorithm ) . Hidden Markov model  and visible Markov
# N/V/J N/R D   N/V/J   . N/J      N+        . . V/J    NPr    N/V/J+ V/C J       NPr
> model  taggers can    both be   implemented using the Viterbi algorithm . The
# N/V/J+ NPl     NPr/VX I/C  N/VX V/J         V     D+  ?       N         . D
> rule - based Brill tagger is unusual in      that       it     learns a   set     of rule patterns , and
# N/V+ . V/J   N/J   N      VL N/J     NPr/J/P N/I/C/Ddem NPr/I+ NPl/V  D/P NPr/V/J P  N/V+ NPl/V+   . V/C
> then  applies those   patterns rather  than optimizing a    statistical quantity .
# N/J/C V       I/Ddem+ NPl/V+   NPr/V/J C/P  V          D/P+ J+          N+       .
>
#
> Many     machine learning methods have also been applied to the problem of POS
# N/I/J/D+ N/V     V+       NPl/V+  N/VX W?   N/V  V/J     P  D   N/J     P  N+
> tagging . Methods such as  SVM , maximum entropy classifier , perceptron , and
# N/V+    . NPl/V+  N/I  N/R ?   . N/J     N       N          . N          . V/C
> nearest - neighbor have all     been tried , and most  can    achieve accuracy above
# W?      . N/V/J/Am N/VX N/I/J/C N/V  V/J   . V/C N/I/J NPr/VX V       N+       N/J/P
> 95 % . [ citation needed ]
# #  . . . N+       V/J+   .
>
#
> A   direct comparison of several methods is reported ( with references ) at  the ACL
# D/P V/J    N          P  J/D+    NPl/V+  VL V/J      . P    NPl/V+     . N/P D+  N+
> Wiki . This    comparison uses  the Penn tag  set     on  some  of the Penn Treebank data ,
# N/V+ . I/Ddem+ N+         NPl/V D+  NPr+ N/V+ NPr/V/J J/P I/J/R P  D+  NPr+ ?        N+   .
> so      the results are directly comparable . However , many    significant taggers are
# N/I/J/C D+  NPl/V+  V   R/C      N/J+       . C       . N/I/J/D N/J         NPl     V
> not included ( perhaps because of the labor        involved in      reconfiguring them   for
# N/C V/J      . N       C/P     P  D+  NPr/V/Am/Au+ V/J      NPr/J/P V             N/IPl+ C/P
> this    particular dataset ) . Thus , it     should not be   assumed that       the results
# I/Ddem+ N/J+       N       . . N    . NPr/I+ VX     N/C N/VX V/J     N/I/C/Ddem D+  NPl/V+
> reported here  are the best     that        can    be   achieved with a    given    approach ; nor even
# V/J      N/J/R V   D   NPr/VX/J N/I/C/Ddem+ NPr/VX N/VX V/J      P    D/P+ N/V/J/P+ N/V+     . N/C N/V/J
> the best      that        have been achieved with a    given    approach .
# D+  NPr/VX/J+ N/I/C/Ddem+ N/VX N/V  V/J      P    D/P+ N/V/J/P+ N/V+     .
>
#
> In      2014 , a    paper  reporting using the structure regularization method for
# NPr/J/P #    . D/P+ N/V/J+ V         V     D+  N/V+      N              N/V    C/P
> part  - of - speech tagging , achieving 97.36 % on  a   standard benchmark dataset .
# N/V/J . P  . N/V    N/V     . V         #     . J/P D/P N/J+     N/V+      N       .
